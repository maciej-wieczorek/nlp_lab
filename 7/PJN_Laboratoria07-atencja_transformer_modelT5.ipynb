{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mooJ5vGuciH3"
      },
      "source": [
        "# Atencja — implementacja atencji między danym stanem dekodera a stanami kodera\n",
        "\n",
        "Poniżej znajdują się dwie macierze `encoder_states` i `decoder_states` reprezentujące stan warstwy ukrytej po przetworzeniu każdego słowa przez koder oraz statyczne wektory zanurzeń związane z danym wejściem dekodera. Pojedynczy stan warstwy ukrytej zawiera zanurzenia o długości = 3, która to liczba jest równa rozmiarowi zanurzenia w dekoderze. W koderze mamy 4 stany warstw ukrytych, ponieważ przetwarzamy sekwencję składającą się z 4 tokenów.\n",
        "\n",
        "W dekoderze znajduje się 5 tokenów, które są generowane na podstawie sekwencji przetwarzanej przez koder.\n",
        "\n",
        "*Zadanie (1 punkt)*\n",
        "\n",
        "Zadanie polega na: a) Obliczeniu podobieństwa wszystkich zanurzeń z dekodera (zapytań -- queries) do wszystkich zanurzeń kolejnych stanów kodera (kluczy -- keys) (pamiętaj, że macierze można transponować. W NumPy transponujemy macierz za pomocą `nazwa_macierzy. T`)\n",
        "\n",
        "a) Softmax (zaimportowany z scipy) należy wykonać na utworzonej macierzy podobieństw. Uwaga: pamiętaj, aby zastosować softmax we właściwym wymiarze. Wszystkie ukryte stany kodera powinny być zmiękczone w kontekście danego stanu dekodera. W scipy funkcja softmax zawiera argument `axis`, który może pomóc.\n",
        "\n",
        "b) Połącz macierz uwagi z kroku b) i „encoder_states”, aby wygenerować macierz zawierającą wektory kontekstu dla każdego tokena z dekodera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Vbq0QW2td41r"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 8.8800e-01,  7.8200e-01,  3.1360e+00],\n",
              "       [-1.6629e+01,  2.4000e-02,  3.9600e+00],\n",
              "       [ 9.3024e+01,  8.4600e-01,  1.3200e-01],\n",
              "       [ 1.6400e+00,  2.2610e+00,  6.0000e-01],\n",
              "       [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "# scipy.special.softmax(x, axis=None)\n",
        "\n",
        "encoder_states = np.array(\n",
        "    [[1.2, 3.4, 5.6],   # encoder's hidden layer output at the step 1, related to a given input token, e.g., I\n",
        "    [-2.3, 0.2, 7.2],   # encoder's hidden layer output at the step 2, related to a given token, e.g., like\n",
        "    [10.2, 0.2, 0.3],   # encoder's hidden layer output at the step 3, related to a given token, e.g., NLP\n",
        "    [0.4, 0.7, 1.2],    # encoder's hidden layer output at the step 4, related to a given token, e.g., \".\"\n",
        "    [0.0, 0.0, 0.0]] # EOS?????????\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "decoder_states = np.array(\n",
        "    [[0.74, 0.23, 0.56],  # decoder's static word embedding at the step 1, related to a given token, e.g., <BOS>\n",
        "    [7.23, 0.12, 0.55],  # decoder's static word embedding at the step 2, related to a given token, e.g., Ja\n",
        "    [9.12, 4.23, 0.44], # decoder's static word embedding at the step 3, related to a given token, e.g., lubię\n",
        "    [4.1, 3.23, 0.5],    # decoder's static word embedding at the step 4, related to a given token, e.g., przetwarzanie\n",
        "    [5.2, 3.1, 8.5]]     # decoder's static word embedding at the step 5, related to a given token, e.g., języka\n",
        ")\n",
        "\n",
        "encoder_states * decoder_states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8bfd5X4fAJq"
      },
      "source": [
        "Oczekiwane wyjścia:\n",
        "\n",
        "a) [[ 4.806 2.376 7.762 1.129] [ 12.164 -12.645 73.935 3.636] [ 27.79 -16.962 94.002 7.137] [ 18.702 -5.184 42.616 4.501] [ 64.38 49.86 56.21 14.45 ]]\n",
        "\n",
        "b) [[4.91780633e-02 4.32948093e-03 9.45248312e-01 1.24414389e-03] [1.49003187e-27 2.50486173e-38 1.00000000e+00 2.94803216e-31] [1.75587568e-29 6.44090821e-49 1.00000000e+00 1.88369172e-38] [4.11416552e-11 1.74069934e-21 1.00000000e+00 2.79811669e-17] [9.99716568e-01 4.94220792e-07 2.82937800e-04 2.06801368e-22]]\n",
        "\n",
        "c) [[ 9.69108631 0.35799187 0.59163688] [10.2 0.2 0.3 ] [10.2 0.2 0.3 ] [10.2 0.2 0.3 ] [ 1.20254471 3.39909302 5.59850122]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sPUDVw7fKHJ"
      },
      "source": [
        "# Transformer\n",
        "## Wykorzystanie modelu T5 opartego na transformerze do rozwiązywania różnych zadań NLP.\n",
        "\n",
        "Dzisiaj poznamy nową bibliotekę — HuggingFace **transformers** (https://huggingface.co/docs/transformers/index) i użyjemy jej do rozwiązania kilku nieoczywistych problemów związanych z NLP za pomocą modelu **T5** .\n",
        "\n",
        "\n",
        "HuggingFace transformers to jedna z najpopularniejszych bibliotek dostarczających nam wysokopoziomowe API do wykorzystania sieci neuronowych do rozwiązywania zadań związanych z przetwarzaniem języka naturalnego, przetwarzaniem dźwięku, wizją komputerową, a nawet scenariuszami multimodalnymi, w których musimy wykorzystywać wiele modalności naraz (np. odpowiadając na pytania o zdjęcia, wydobywając informacje z faktur).\n",
        "\n",
        "Najpierw zainstalujmy zależności, samą bibliotekę `transformers` oraz moduł `sentencepiece`, który pomaga nam tokenizować dokumenty i przekształcać tokeny w kodowanie one-hot (ideę sentencepiece omówimy szczegółowo później).\n",
        "\n",
        "**Ostrzeżenie**: jeśli zauważysz jakieś dziwne wyjątki, takie jak `cannot call from_pretrained na obiekcie None` gdzieś w twoim kodzie, zrestartuj środowisko używając: Runtime -> restart. Następnie uruchom komórki z kodem (bez ponownej instalacji bibliotek) jeszcze raz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jmRzPimhfRja"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#'\n",
            "WARNING: You are using pip version 21.1.3; however, version 23.0.1 is available.\n",
            "You should consider upgrading via the 'c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n",
            "ERROR: Invalid requirement: '#'\n",
            "WARNING: You are using pip version 21.1.3; however, version 23.0.1 is available.\n",
            "You should consider upgrading via the 'c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "# !pip install transformers   # install HuggingFace transformers library\n",
        "# !pip install sentencepiece  # install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSAg0LOEgGj4"
      },
      "source": [
        "Przeczytaj dokumentację dotyczącą modelu T5 dostępne tutaj: https://huggingface.co/docs/transformers/model_doc/t5\n",
        "\n",
        "W sekcji `wnioskowanie` znajdziesz opis pokazujący w jaki sposób możemy pobrać wytrenowany model i użyć go do rozwiązania zadanego zadania. Po prostu użyj dostarczonego kodu, aby przetłumaczyć jakieś zdanie z angielskiego na niemiecki!\n",
        "\n",
        "*(0.5 punkta)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "lYOfsrkeJ6GF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maciej\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Maciej\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Das Haus ist wunderbar.', 'Ich arbeite gerne in NYC.', 'Alice hat eine Katze']\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# TODO: instantiate T5 and use it to solve some task\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "task_prefix = \"translate English to German: \"\n",
        "# use different length sentences to test batching\n",
        "sentences = [\"The house is wonderful.\", \"I like to work in NYC.\", \"Alice has a cat\"]\n",
        "\n",
        "inputs = tokenizer([task_prefix + sentence for sentence in sentences], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "output_sequences = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        "    do_sample=False,  # disable sampling to test if batching affects output\n",
        ")\n",
        "print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z322IklnhQnO"
      },
      "source": [
        "## Różne zadania\n",
        "\n",
        "Eksperymentuj z innymi danymi wejściowymi, np. tymi, które przedstawiono na rysunku 1 przedstawionym w artykule przedstawiającym model T5 lub nawet szerszą listą przypadków użycia z Dodatku D dostarczonego z artykułem. Artykuł można znaleźć tutaj: https://arxiv.org/pdf/1910.10683.pdf\n",
        "\n",
        "Uwaga: wśród dostarczonych danych wejściowych zastosowano kilka skrótów, niektóre z nich to:\n",
        "- `stsb`: oznacza semantyczny test porównawczy podobieństwa tekstu. Biorąc pod uwagę dwa zdania, możemy obliczyć ich podobieństwa semantyczne, co może pomóc nam ustalić, czy jedno zdanie jest parafrazą drugiego.\n",
        "- `cola`: oznacza Corpus of Linguistic Acceptability i pomaga nam określić, czy dane zdanie jest gramatyczne, czy niegramatyczne.\n",
        "\n",
        "Jeśli spojrzysz na Dodatek D, skrótów jest więcej, są one związane z nazwami zadań przedstawionymi w benchmarku GLUE (dostępny tutaj: https://gluebenchmark.com/tasks) i benchmarku SUPERGLUE (dostępny tutaj: https:/ /super.gluebenchmark.com/tasks). Ideą GLUE i SUPERGLUE jest zebranie zestawu trudnych zadań, które można wykorzystać do oceny systemów wymagających rozumienia języka naturalnego.\n",
        "\n",
        "**Wklej 3 przykładowe zadania i przetworzone dane wejściowe w komórce poniżej (1 punkt)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OfxmQwV4lIXw"
      },
      "outputs": [],
      "source": [
        "# TODO: paste 3 calls returning the outputs generated by T5 for some selected inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQvUkC-BlW3q"
      },
      "source": [
        "## Różne typy modeli\n",
        "\n",
        "Dostępnych jest kilka modeli T5, które różnią się wielkością (i jakością). Im większy model, tym lepsze wyjście powinien generować. Eksperymentuj z niektórymi modelami z następującego zestawu:\n",
        "- t5-small\n",
        "- base t5\n",
        "- t5-large\n",
        "- t5-3b\n",
        "- t5-11b\n",
        "\n",
        "Sprawdź, czy można zaobserwować różnicę w jakości generowanych tekstów.\n",
        "\n",
        "Porównaj również rozmiar modeli, możesz użyć funkcji `model.num_parameters()`, aby uzyskać numer parametru związany z każdym modelem. Dla każdego modelu, który jesteś w stanie załadować, podaj rozmiar w poniższej komórce (jeśli nie możesz załadować danego modelu, bo jest za duży, bez obaw, po prostu wpisz 'too big to load'). (*2 punkty*) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1MTMhyyeR3ZJ"
      },
      "outputs": [],
      "source": [
        "# t5-small params number:\n",
        "# t5-base params number:\n",
        "# t5-large params number:\n",
        "# t5-3b params number:\n",
        "# t5-11b params number:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDud66l6msUI"
      },
      "source": [
        "## T5 specyficzne dla języka (ZADANIE OPCJONALNE – nie musisz tutaj podawać kodu)\n",
        "\n",
        "Istnieją nawet alternatywy dla oryginalnych modeli T5. Ponieważ model T5 był trenowany na języku angielskim, dostępne są modele specyficzne dla innych języków, np. polskiego (np. plT5 zaproponowany przez Allegro - https://huggingface.co/allegro/plt5-small). Polski model został przeszkolony do rozwiązywania zestawu zadań zebranych w benchmarku KLEJ, który stanowi polską analogię do benchmarku GLUE: https://klejbenchmark.com.\n",
        "\n",
        "Więcej szczegółów na temat plT5 można znaleźć w artykule badawczym: https://arxiv.org/pdf/2205.08808.pdf. Tabela 2 przedstawia przykładowe podpowiedzi, które można wykorzystać do rozwiązania niektórych zadań wymienionych w KLEJ.\n",
        "\n",
        "Możesz wyszukać alternatywę dla oryginalnego T5, na przykład tę związaną z Twoim językiem, i poeksperymentować z nią (**to zadanie nie jest obowiązkowe**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KJZZUkduoEhZ"
      },
      "outputs": [],
      "source": [
        "# (OPTIONAL): If you want, experiment with some alternative models (like language-related, e.g., plT5 related to Polish)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1hsTnxboIe8"
      },
      "source": [
        "## Flan-T5\n",
        "\n",
        "Pod koniec 2022 roku zaproponowano ewolucję T5 o nazwie Flan-T5. Ten model jest również dostarczany przez bibliotekę transformatorów HuggingFace. Odwiedź tę stronę: https://huggingface.co/docs/transformers/model_doc/flan-t5, aby zobaczyć, jak możesz użyć tego modelu (wystarczy zmienić nazwę modelu, aby pobrać!).\n",
        "\n",
        "Flan-T5 jest znacznie potężniejszy niż T5. Możesz zajrzeć do Dodatku D zawartego w artykule opisującym Flan T5, aby zapoznać się z niektórymi formatami wejściowymi (monitami) i generowanymi wartościami. Artykuł jest tutaj: https://arxiv.org/pdf/1910.10683.pdf. Powinieneś skupić się na polach „przetworzonych danych wejściowych”, ponieważ są to reprezentacje używane przez model. Eksperymentuj z wybranymi zadaniami i sprawdź, czy możesz uzyskać takie same wyniki! W poniższym kodzie wklej kod ładujący model Flan-T5 i wykorzystujący go do rozwiązania wybranych zadań. (*1 punkt*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ppbmGGqVqERf"
      },
      "outputs": [],
      "source": [
        "# TODO: Here paste some code using Flan-T5 model to solve some task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwyRmBx6q0iT"
      },
      "source": [
        "## (OPCJONALNIE) Dostrajanie\n",
        "\n",
        "Możesz nawet dostroić model T5/Flan-T5, aby rozwiązać wybrane zadanie. Możesz załadować istniejący model T5/Flan-T5, który jest już przeszkolony do rozwiązywania niektórych zadań, i wykorzystać moc 'transfery wiedzy', aby nauczyć go rozwiązywać różne zadania. Jest to o wiele lepsze niż trenowanie sieci od zera i powinno wymagać mniejszej liczby przykładów szkoleniowych.\n",
        "\n",
        "Faza dostrajania jest dość złożona. Jednak opis krok po kroku można znaleźć tutaj: https://www.philschmid.de/fine-tune-flan-t5\n",
        "\n",
        "Możesz spróbować dostroić wybrany model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frt5p4E_rjGd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
