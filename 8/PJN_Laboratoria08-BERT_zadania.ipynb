{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT\n",
        "\n",
        "Dziś omówimy BERTa -- model, który transformuje nasze tokeny w embeddingi kontekstowe o bardzo dobrej jakości (lepszej niż poprzednie metody, jednak za cenę czasu przetwarzania).\n",
        "\n",
        "Wykorzystamy dziś bibliotekę `transformers` dlatego zainstalujmy ją najpierw:"
      ],
      "metadata": {
        "id": "rhIZIpEnr8P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "eKMUJ2X5AXQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT\n",
        "\n",
        "## Zadanie 1 (1 punkt): Tokenizacja przy użyciu algorytmu WordPiece\n",
        "\n",
        "Poniżej znajduje się prosty fragment kodu, który pobiera model `bert-base` (wersja `uncased` - ten model wprowadził etap wstępnego przetwarzania danych, który przekształcał każdy tekst na tekst pisany małymi literami) i tworzy instancję odpowiedniego tokenizatora dla tego modelu. O tym konkretnym modelu można dowiedzieć się tutaj: https://huggingface.co/bert-base-uncased (Zachęcam do przeczytania opisu! Wiele modeli hostowanych na stronie huggingface ma świetną dokumentację i zawsze warto ją sprawdzić).\n",
        "\n",
        "Następnie w linii 4 definiujemy tekst do tokenizacji, uruchamiamy tokenizer w linii 5 i używamy tokenizowanych danych wejściowych jako danych wejściowych do modelu BERT, który jest wywoływany w linii 6.\n",
        "\n",
        "Uruchom poniższy kod. Wygenerowaliśmy osadzania BERT przy użyciu 6 linii kodu! ;)\n",
        "\n",
        "Jeśli uda sie uruchomić ten kod -- gratulacje, to wystarczy na 1 punkt ;)."
      ],
      "metadata": {
        "id": "qr8B0bjk-Y-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)"
      ],
      "metadata": {
        "id": "7U5MBbap-cOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uruchomienie tokenizatora przez zwykłe wywołanie obiektu tokenizatora (`tokenizer(text, return_tensor='pt')`) zwraca słownik pythonowy zawierający 3 klucze: `input_ids`, `token_type_ids` i `attention_mask`. Na razie skupmy się tylko na `input_ids`.\n",
        "\n",
        "Możesz odwiedzić stronę internetową: https://huggingface.co/docs/transformers/glossary, aby dowiedzieć się więcej o roli `token_type_ids` i `attention_mask`.\n",
        "\n",
        "\n",
        "`input_ids` to lista list (reprezentowana jako tensor). Lista zewnętrzna gromadzi dokumenty, podczas gdy listy wewnętrzne gromadzą tokeny w tym dokumencie. Tutaj przetworzyliśmy tylko jeden dokument (zdanie), więc jest tylko jedna „zewnętrzna” lista.\n",
        "\n",
        "Każda z wewnętrznych list zawiera sekwencję identyfikatorów. To są pozycje tokenów w słowniku. Można ich użyć do generowania reprezentacji one-hot encoding dla naszych słów, ponieważ znając długość słownika i znając pozycję danego tokena w słowniku, możemy wygenerować wektor długości słownika, który jest wypełniony zerami , następnie ustawiamy wartość przypisaną do pozycji tokena na 1, aby wygenerować kodowanie typu one-hot.\n",
        "\n",
        "Aby wygenerować identyfikatory, musimy najpierw dokonać tokenizacji naszego tekstu.\n",
        "\n",
        "Te identyfikatory wymagają mniej pamięci niż przechowywanie tokenów jako łańcuchów znaków (np zamiast słowa \"najciekawszy\" będziemy mieć jedną liczbę, np. 2045)!\n",
        "\n",
        "Uruchom poniższy kod, aby zobaczyć jakie wyjście wygeneruje tokenizator. Należy pamiętać, że liczba identyfikatorów tokenów wygenerowanych przez tokenizator nie jest równa liczbie tokenów w sekwencji. Za chwilę zobaczymy, dlaczego tak jest."
      ],
      "metadata": {
        "id": "v6lcMqMBJBAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input"
      ],
      "metadata": {
        "id": "Zm01ajH4_-H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identyfikatory generowane przez tokenizator są liczbami, które ciężko nam zrozumieć. Jednak ponieważ tokenizator zawiera mapowanie zamieniające tokeny na ich pozycje, możemy odwrócić ten proces.\n",
        "\n",
        "Pierwszy wiersz kodu pobiera identyfikatory zdefiniowane dla pierwszego zdania.\n",
        "Następnie wywołujemy `convert_ids_to_tokens`, aby przekształcić te identyfikatory w tokeny. Uruchom kod i przeanalizuj wyjście."
      ],
      "metadata": {
        "id": "-ZoJWPXcqJzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_sentence_ids = encoded_input['input_ids'][0]\n",
        "tokenizer.convert_ids_to_tokens(first_sentence_ids)"
      ],
      "metadata": {
        "id": "JYIdWHlxAnIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow, widzimy, że tokenizator nie tylko tokenizuje nasz tekst (dzieli na tokeny), ale także generuje specjalne tokeny wymagane przez BERT ([SEP] i [CLS])! Należy pamiętać, że po tokenizacji nie mamy wielkich liter w naszych tokenach. Jest to spowodowane użyciem modelu `bert-base-uncased`. Ponieważ model został przeszkolony na danych pisanych małymi literami, tokenizator zapewnia również, że tokeny są zamienione do postaci zawierającej jedynie małe litery.\n",
        "\n",
        "\n",
        "## Subword units -- Jednostki podrzędne\n",
        "\n",
        "Jednak długość wygenerowanych identifykatorów -- `input_ids` może być jeszcze większa w stosunku do długości naszego tekstu. Czasami słownik mdoelu nie zawiera danego słowa w całości. Ponieważ BERT używał tokenizacji WordPiece, do obsługi takich przypadków tokenizator próbuje podzielić te słowa na mniejsze fragmenty, które są przechowywane w naszym słowniku. Wykonajmy tokenizację dokumentu zawierającego rzadkie słowa i zobaczmy jaki wynik otrzymamy."
      ],
      "metadata": {
        "id": "NKQJ9jH2qp07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_output = tokenizer(['NVIDIA DGX A100'])\n",
        "\n",
        "input_ids = tokenizer_output['input_ids'][0]\n",
        "input_ids"
      ],
      "metadata": {
        "id": "fZTIXMpDAqwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otrzymaliśmy całkiem sporo tokenów jak na tak krotki tekst! Sprawdźmy co one reprezentują:"
      ],
      "metadata": {
        "id": "VozPQjo7ruWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens(input_ids)"
      ],
      "metadata": {
        "id": "dhvna3O5Asl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jak widzimy, słownik przypisany do `bert-base` nie zawiera tokenów takich jak \"nvidia\", \"dgx\" i \"a100\", dlatego są one podzielone na jednostki podrzędne (subword units).\n",
        "\n",
        "Za każdym razem, gdy dany subword unit rozpoczyna się podwójnym haszem (##), wiemy, że jest on kontynuacją poprzedniego tokenu (subword unitu).\n",
        "\n",
        "Możemy użyć tych informacji do zrekonstruowania oryginalnego tekstu, łącząc te jednostki podrzędne (czasami nazywane subtokenami) w pełne tokeny. Możemy osiągnąć ten cel za pomocą następującego wiersza kodu:"
      ],
      "metadata": {
        "id": "4e9XJATrtKaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(input_ids)"
      ],
      "metadata": {
        "id": "_SfeJFf4BU8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jeśli wiemy, jak mapować tokeny do ich pozycji w słowniku, jedyną brakującą częścią jest określenie, jak długi powinien być nasz wektor one-hot-encoding (lub jak duży jest nasz słownik).\n",
        "\n",
        "To przekształcenie identyfikatorów w kodowanie one-hot jest wykonywane automatycznie przez bibliotekę `transformer`. Możesz jednak łatwo sprawdzić rozmiar słownika, używając następującego wiersza kodu:"
      ],
      "metadata": {
        "id": "78qbqsgItqoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "oD4qXpxNB9fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zadanie 2 (4 punkty): Wykorzystanie wstępnie przeszkolonego BERT do wygenerowania cech, które można wykorzystać do rozwiązania zadania klasyfikacyjnego.\n",
        "\n",
        "Jak omówiliśmy podczas wykładu, możemy wygenerować wektor o stałej długości reprezentujący dowolne wejście, biorąc jedynie embedding utworzony dla tokena `[CLS]` (który jest reprezentacją całej sekwencji).\n",
        "\n",
        "W tym zadaniu Twoim celem jest użycie wstępnie wytrenowanego modelu BERT w celu uzyskania reprezentacji dla danego zestawu danych. Następnie reprezentacje te zostaną użyte do trenowania modelu regresji logistycznej.\n",
        "\n",
        "Postaramy się wygenerować rozwiązanie, które wykryje, czy dana recenzja jest pozytywna, czy nie!\n",
        "\n",
        "W tym celu użyjemy wariantu BERT o nazwie `distilBERT`. DistilBERT to mniejszy model BERT zachowujący prawie pełną jakość oryginalnego BERTa. W rzeczywistości jest to skompresowany model BERT. Zachowuje się tak samo, ale jest destylowany, więc naszym celem było osiągnięcie podobnej jakości przy mniejszych parametrach.\n",
        "\n",
        "Postępuj zgodnie z samouczkiem, który znajdziesz tutaj: https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb (jest nawet post na blogu dotyczący ten samouczek: http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)\n",
        "\n",
        "\n",
        "Umieść fragmenty kodu z tego tutorialu tu i obserwuj wyniki. Jeśli uda ci się przejść przez ten tutorial - otrzymasz 4 punkty. "
      ],
      "metadata": {
        "id": "vTw0zxjwuctL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### -------------------------------------------------------------\n",
        "### Umiesc kod tu\n",
        "### -------------------------------------------------------------"
      ],
      "metadata": {
        "id": "wlRMuDYsw65f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zadanie 3 (opcjonalne — niewymagane): dostrajanie BERT\n",
        "\n",
        "W powyższym przykładzie model BERT (a dokładniej jego mniejszy członek rodziny: distilBERT) został użyty tylko do dostarczenia wektorów reprezentujących całe dokumenty.\n",
        "\n",
        "Teraz chcielibyśmy dostroić istniejący model BERT. Jak omówiliśmy podczas wykładu, możemy to osiągnąć, po prostu przełączając górną warstwę sieci. Zamiast rozwiązywać zadania Masked Language Model i Next Sentence Prediction, możemy dodać własną warstwę klasyfikacyjną (nazywaną też głową klasyfikacji) i wyszkolić całą naszą sieć do rozwiązania danego zadania.\n",
        "\n",
        "Jest świetny i łatwy do naśladowania samouczek dotyczący dostrajania, dostępny tutaj: https://github.com/huggingface/notebooks/blob/main/transformers_doc/training.ipynb\n",
        "\n",
        "Jeśli chcesz, skorzystaj z tego samouczka (możesz skopiować i wkleić fragmenty kodu z notatnika tutaj)."
      ],
      "metadata": {
        "id": "qFGIZAN6xEsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### -------------------------------------------------------------\n",
        "### Umieść kod tu\n",
        "### -------------------------------------------------------------"
      ],
      "metadata": {
        "id": "8papxHiBxHlF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}