{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a0w5N-ULYslb"
      },
      "source": [
        "# Generowanie tekstu za pomocą GPT-2\n",
        "\n",
        "Dzisiaj spróbujemy wygenerować teksty z wykorzystaniem modelu GPT-2 zaproponowanego przez OpenAI. Model ten jest pochodną transformera. Podobnie jak BERT jest jego fraagmentem -- o ile BERT jest koderem z transformera, o tyle GPT -- dekoderem. GPT-2 to wstępnie wytrenowany model, który można pobrać i używać w taki sam sposób jak BERT.\n",
        "\n",
        "Tutaj możesz znaleźć świetne wprowadzenie do ogólnej idei GPT-2: https://jalammar.github.io/illustrated-gpt2/\n",
        "\n",
        "Generalnie jest to model językowy, model, który daje nam prawdopodobieństwo tego, że dany token jest kontynuacją zadanego kontekstu. Na przykład mając następujący kontekst: „Ala ma pięknego”, GPT-2 może oszacować, że istnieje 5%” szans, że następnym słowem będzie kota, i „0,0001%”, że następnym słowem będzie „ma”. `.\n",
        "\n",
        "Wykorzystamy bibliotekę `Huggingface Transformers` do eksperymentowania z GPT-2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7LwgDoMiINYf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.28.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2023.3.22)\n",
            "Requirement already satisfied: requests in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: colorama in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2022.9.24)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IrgKFTxbasR0"
      },
      "source": [
        "# PODSTAWOWE GENEROWANIE TEKSTU (2 punkty)\n",
        "\n",
        "Zacznijmy od podstawowego scenariusza — ponieważ GPT-2 może obliczyć prawdopodobieństwo wystąpienia następnego słowa po zadanym kontekście, może być używany do generowania tekstów. W bibliotece `transformers` możemy to zrobić dość łatwo. `transformers` zapewnia dostarcza tak zwane potoki, które ukrywają wszystkie warstwy abstrakcji, dzięki czemu możemy generować teksty za pomocą dwóch linii kodu.\n",
        "\n",
        "Przeczytaj dokumentację, która znajduje się tutaj: https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/pipelines, aby zapoznać się z potokami.\n",
        "\n",
        "Następnie wypełnij poniższy kod odpowiednimi fragmentami. W linii 2 skonstruujmy potok typu `text-generation` i ustawmy parametr `model` na `gpt2`.\n",
        "\n",
        "Następnie `generator` można wywoływać w taki sam sposób jak funkcję dając po nim nawiasy okrągłe z parametrami `generator(__tutaj  parametry__)`. Po prostu podaj kilka pierwszych słów tekstu w formie napisu (string) jako pierwszy argument pozycyjny (nie dodawaj spacji na końcu). Możesz podać dodatkowe parametry, takie jak `max_length` (aby ograniczyć długość generowanego tekstu) lub `num_return_sequences` (aby zmusić GPT-2 do wygenerowania wielu tekstów)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "D7QxEKB_IOZ1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alice has a lot of things going for her,\" Miller replied: \"And he's not going to be able to compete on that stage, as long\n",
            "Alice has a short temper and is often very bitter toward others. He has an abusive past, as well as a strong desire on his part to leave\n",
            "Alice has a habit of taking pictures while doing things that seem silly so that she can just go and make herself look normal. But even that is really\n",
            "\n",
            "#########\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alice has a bit more of a problem. Her mother recently died of cancer. They have been separated for the past few years and were divorced a week\n",
            "Alice has a problem with her boyfriend, Jake Farenthold, who she'd been dating for eight months. What's she doing? A short\n",
            "Alice has a very, very good understanding of technology\", he says.\n",
            "\n",
            "\"There's a tremendous amount of interest out there about Bitcoin, whether\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\") # construct text-generation pipeline with model set to gpt2\n",
        "outputs = generator(\"Alice has a\", max_length=30, num_return_sequences=3)      # uncomment and add parameters\n",
        "for output in outputs:\n",
        "    print(output['generated_text'])\n",
        "print(\"\\n#########\\n\")\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2-large\")\n",
        "generator(\"Alice has a\", max_length=10, num_return_sequences=3)\n",
        "outputs = generator(\"Alice has a\", max_length=30, num_return_sequences=3)\n",
        "for output in outputs:\n",
        "    print(output['generated_text'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3tE7Utv2j3k8"
      },
      "source": [
        "Istnieją różne modele oparte na GPT, które są dostępne w bibliotece `transformers`. Tutaj: https://huggingface.co/models?search=gpt, znajdziesz ich listę. Różnią się zestawami danych, na których zostały wytrenowane (oryginalny GPT-2 został wytrenowany na Webtext https://paperswithcode.com/dataset/webtext, który składa się z ~ 40 GB tekstów ściągniętych z Internetu) i rozmiarami modeli (np. GPT2-small składa się z 117M parametrów, GPT2-medium z 345M, GPT2-large z 762M).\n",
        "\n",
        "W zależności od naszych potrzeb i dostępnej pamięci GPU, możemy wybrać odpowiednią wielkość.\n",
        "Istnieją również modele destylowane, które są `skompresowane` podobnie jak np. popularny model DistilBERT: https://huggingface.co/distilgpt2 (Więcej o destylacji znajdziesz tutaj: https://neptune.ai/blog/knowledge-distillation).\n",
        "\n",
        "Sprawdź jak modele o różnej wielkości mają się do jakości generowanych tekstów. Użyj `gpt2-small`, `gpt2-medium`, `gpt2-large` zamiast `gpt2` w potoku i przeanalizuj wyniki.\n",
        "\n",
        "Sprawdź, jak działają modele wytrenowane na bardziej „konkretnych” danych (np.\n",
        "`CodeGPT-small-java-adaptedGPT2`, którego można użyć do pisania kodu w Javie)\n",
        "\n",
        "* Raport z wyników nie jest wymagany. Po prostu poeksperymentuj, jeśli interesuje Cię ten temat :)*\n",
        "\n",
        "\n",
        "# GPT-2 jako źródło wiedzy\n",
        "Ponieważ model dostarcza prawdopodobnych kontynuacji tekstów, możemy na ich podstawie znaleźć odpowiedzi na niektóre pytania.\n",
        "Możesz wpisać `Stolicą Polski jest` jako kontekst, aby sprawdzić, czy zostanie zaproponowana `Warszawa`.\n",
        "\n",
        "(uwaga!: nie dodawaj spacji na końcu kontekstu. Często prowadzi to do... dziwnych rezultatów)\n",
        "\n",
        "Pamiętaj jednak, że internet jest stronniczy. Istnieje wiele prac poświęconych badaniu stronniczości (biasu) modeli GPT. Ten artykuł jest łatwą do przyswojenia analizą problemu http://aclanthology.lst.uni-saarland.de/D19-1339.pdf. Ponieważ te modele są trenowane na treściach generowanych przez ludzi, nie powinniśmy traktować ich jak wyroczni. Zamiast tego powinniśmy traktować je jako pewien model stereotypowego człowieka ;).\n",
        "\n",
        "**Zadanie 2 (1 punkt)\n",
        "Wpisz 5 przykładów wejść, które pokażą potencjalny bias modelu (np. kobieta = nauczycielka, mężczyzna=mechanik)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "v6FQrhOeIleW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A woman works as\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<output>A woman works as a care aide in a maternity ward at an unspecified hospital (AFP Photo/Nigde Doyen)\n",
            "\n",
            "Brus</output>\n",
            "<output>A woman works as a waitress in the restaurant. She said the owners had a rule to ban people with no ID from smoking in the restaurant. She</output>\n",
            "<output>A woman works as a librarian at the University of Toronto library. She's on her way to an office on the third floor of her building when</output>\n",
            "<output>A woman works as a receptionist at a bank, and is having trouble with her health insurance. In exchange for her good-paying job, she</output>\n",
            "<output>A woman works as a nurse at a maternity hospital in Kabul. Taliban fighters have been known to rape and enslave women in the country. REUTERS/</output>\n",
            "####################\n",
            "A man works as\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<output>A man works as a guard near a fire station in the suburbs of Barcelona, Spain January 23, 2017. REUTERS/Albert Gea\n",
            "\n",
            "Three</output>\n",
            "<output>A man works as a tailor in this picture illustration taken in Zenica, Bosnia and Herzegovina, May 7, 2008. REUTERS/O</output>\n",
            "<output>A man works as a guard at a detention facility in the southern Mexican city of Manzanillo Feb. 8. A judge in April ordered Mexico to</output>\n",
            "<output>A man works as he waits in her bathroom. AP 16/17 Customers react as they near a soda shop in London Getty Images 17/17 A</output>\n",
            "<output>A man works as a tailor, by day. He is a devout Muslim. He has an eight children, aged between 12 and 25. A little</output>\n",
            "####################\n",
            "The Black man worked as\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<output>The Black man worked as an electrician, while the white man spent time in farm labor.\"\n",
            "\n",
            "(Emphasis added.) White vs. Black</output>\n",
            "<output>The Black man worked as a truck driver, selling tickets and cigarettes in the street and selling liquor in bars. He was also involved in gambling at various</output>\n",
            "<output>The Black man worked as a driver, according to Fox News. He was killed after being rear-ended by a car traveling south on Interstate 95 near</output>\n",
            "<output>The Black man worked as a laborer before a friend brought him to New Orleans in 1856. There, he was forced to clean water, and</output>\n",
            "<output>The Black man worked as the owner of a small electronics store in East Cleveland. According to court documents, he was on probation for operating a vehicle while</output>\n",
            "####################\n",
            "The White man worked as\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<output>The White man worked as a barber.\n",
            "\n",
            "The Black man worked as a bus boy.\n",
            "\n",
            "The Red man, of unknown sex,</output>\n",
            "<output>The White man worked as an orderly at a local hospital and then moved out of state, to West Virginia. The White man then settled in Montana,</output>\n",
            "<output>The White man worked as a farmer with his grandmother.\n",
            "\n",
            "Police said the farm had been used as a drug and weapons trafficking operation.\n",
            "\n",
            "</output>\n",
            "<output>The White man worked as an assistant to the chief in the late '60s and has a copy of the original \"Molotov Cocktail</output>\n",
            "<output>The White man worked as a truck driver in California.\"\n",
            "\n",
            "The man was a truck driver? Yeah, how's that going to end?\n",
            "</output>\n",
            "####################\n",
            "The gay person was known for\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<output>The gay person was known for taking part in anti-Islamic rallies.\n",
            "\n",
            "He was arrested for the incident which happened amid the mass protests. He</output>\n",
            "<output>The gay person was known for his outrageous sense of humor.\n",
            "\n",
            "\n",
            "A statement from KOMO News:\n",
            "\n",
            "KOMO News is saddened</output>\n",
            "<output>The gay person was known for his \"love life\" (which, if you're not out of your mind, might cause some confusion). So he</output>\n",
            "<output>The gay person was known for being aggressive. His presence was threatening.\n",
            "\n",
            "Gay people like the phrase 'we are a gay community'. This phrase</output>\n",
            "<output>The gay person was known for being very careful and did not appear willing to do any dirty tricks. He took the other side on that issue. Also</output>\n",
            "####################\n",
            "The straight person was known for\n",
            "<output>The straight person was known for his knowledge, love and ability with a lot of languages. As well as knowledge of other languages, he had the ability</output>\n",
            "<output>The straight person was known for his calm demeanor, he was a man who was known for his good humor and a nice smile on his face. He</output>\n",
            "<output>The straight person was known for never once taking a sip or spitting in his beer-soaked mouth.\n",
            "\n",
            "The man who knew his audience was</output>\n",
            "<output>The straight person was known for his willingness to fight for justice, and it was in his blood to stand up to those who abuse their power and control</output>\n",
            "<output>The straight person was known for his bold statements. However, at any given time of the day, he'd get himself in a tizzy. If</output>\n",
            "####################\n"
          ]
        }
      ],
      "source": [
        "inputs = [\n",
        "    \"A woman works as\",\n",
        "    \"A man works as\",\n",
        "    \"The Black man worked as\",\n",
        "    \"The White man worked as\",\n",
        "    \"The gay person was known for\",\n",
        "    \"The straight person was known for\"\n",
        "]\n",
        "\n",
        "for input_string in inputs:\n",
        "    print(input_string)\n",
        "    outputs = generator(input_string, max_length=30, num_return_sequences=5)\n",
        "    for output in outputs:\n",
        "        print(f'<output>{output[\"generated_text\"]}</output>')\n",
        "    print('#'*20)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7alsiYzOyoCa"
      },
      "source": [
        "# Zachłanne wyszukiwanie vs beam search\n",
        "\n",
        "Domyślny workflow generowania tekstu za pomocą GPT-2 wykorzystuje strategię wyszukiwania zachłannego. Biorąc pod uwagę pewien kontekst, model wybiera token następny patrząc na rozkład prawdopodobieństwa dla tego następnego tokenu. Jednak w tym scenariuszu możemy wygenerować „nieoptymalne” sekwencje. Proszę spojrzeć na tę stronę internetową, aby zrozumieć ideę algorytmu beam search https://huggingface.co/blog/how-to-generate. Krótko mówiąc, beam search zachowuje najbardziej prawdopodobną „liczbę wiązek” hipotez w każdym kroku czasowym i ostatecznie wybiera hipotezę, która ma ogólnie najwyższe prawdopodobieństwo. Dzięki temu jest w stanie spojrzeć nie tylko na bezpośredni następnik, ale również na prawdopodobieństwa kolejnego tokenu.\n",
        "\n",
        "Poniższy kod opisuje alternatywne podejście do korzystania z GPT. Zamiast potoku, tu ręcznie generujemy tokenizator i model, a następnie przekazujemy stokenizowany kontekst do modelu. Proszę spojrzeć na wywołanie funkcji `generate`, można w nim znaleźć parametr `num_beams`, który ustawia liczbę wiązek do zachowania! Spróbuj zmienić ten parametr, aby zobaczyć, jak zmienia się jakość tekstu.\n",
        "\n",
        "**Zadanie 3 (2 punkty): Odpowiedz na pytanie (w komentarzu w kodzie) -- jak parametr num_beams wpływa na jakość tekstu (i dlaczego)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "s6XND_Dk4EF7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The GPT model is great for the first few years, but it's not going to be as\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "starting_context = \"The GPT model is great\"\n",
        "\n",
        "input_ids = tokenizer(starting_context, return_tensors=\"pt\").input_ids\n",
        "\n",
        "\n",
        "outputs = gpt_model.generate(\n",
        "    input_ids,\n",
        "    num_beams=1,\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=1,\n",
        ")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "# num_beams zwiększa jakość generowanego tekstu, ponieważ zamiast wybierać jedno następne słowo,\n",
        "# to rozważa ciąg kilku następnych słów i porównuje kombinacje"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2u-gyiu-5GG9"
      },
      "source": [
        "# Ograniczanie GPT-2\n",
        "\n",
        "Czasami chcielibyśmy ograniczyć kreatywność wyjścia generowanego przez model. Jeśli używasz modelu GPT2 do pisania komentarzy o swoich produktach, chcesz, aby były pozytywne :). Czy nie byłoby ciekawym zmuszenie GPT-2 do generowania tekstów, które muszą zawierać wybrane słowa typu `cudowny`, `najlepszy` czy `niesamowity`? :).\n",
        "\n",
        "Modele GPT-2 pozwalają na takie ograniczanie wygenerowanego wyjścia. Dobre wprowadzenie można znaleźć tutaj: https://towardsdatascience.com/new-hugging-face-feature-constrained-beam-search-with-transformers-7ebcfc2d70e9\n",
        ".\n",
        "\n",
        "Przeanalizuj poniższy fragment kodu (zmodyfikowany kod ze wspomnianej powyżej strony internetowej), aby zobaczyć, jak możemy zmusić GPT-2 do korzystania z niektórych tokenów. Są 2 przypadki:\n",
        "* podajemy jakiś pojedynczy token, który musi znaleźć się gdzieś w generowanym tekście\n",
        "* podajemy listę alternatyw, z których model GPT-2 wybiera jedną.\n",
        "\n",
        "Ważna uwaga: podczas eksperymentowania z kodem zauważyłem kiedyś, że model wygenerował `besting` zamiast oczekiwanego słowa `best`. Na początku byłem zaskoczony, ale działa to dobrze: podczas gdy „best” jest tokenem, którego oczekujemy, że będzie obecny w generowanym tekście, we wstępnie wytrenowanych modelach związanych z transformatorami używamy tokenizacji, która może generować jednostki słów podrzędnych (subword units). Jeśli po `best` zostanie wygenerowany subtoken będący kontynuacją (np. `##ing` zgodnie z notacją WordPiece używaną w BERT), to tokeny te zostaną połączone. To nie powoduje, że wynik jest błędny — token `best` jest zawarty w wygenerowanym tekście!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Q4HFp4rfIstU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The laptop is powered by an Intel Core i7-4790K CPU, which has amazing best\n",
            "The product is available in a variety of colors and sizes, including the standard black. The beautiful best\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "must_contain_token = \"best\"\n",
        "must_contain_alternatives = [\"amazing\", \"wonderful\", \"beautiful\", \"exceptional\"]  # let gpt choose which word to use\n",
        "\n",
        "\n",
        "force_words_ids = [\n",
        "    tokenizer([must_contain_token], add_prefix_space=True, add_special_tokens=False).input_ids,\n",
        "    tokenizer(must_contain_alternatives, add_prefix_space=True, add_special_tokens=False).input_ids,\n",
        "]\n",
        "\n",
        "starting_text = [\"The laptop\", \"The product\"]\n",
        "input_ids = tokenizer(starting_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "\n",
        "outputs = gpt_model.generate(\n",
        "    input_ids,\n",
        "    force_words_ids=force_words_ids,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=1,\n",
        "    remove_invalid_values=True,\n",
        ")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "print(tokenizer.decode(outputs[1], skip_special_tokens=True))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qgPecOwX8svn"
      },
      "source": [
        "W 2020 roku powstała nowa wersja o nazwie GPT3. Chociaż OpenAI nie opublkowało modelu do ściągnięcia, zapewniono dostęp do modelu jedynie za pośrednictwem interfejsu API, podejmowane są pewne próby replikacji modelu. Model, który powinien działać tak samo jak GPT3, znajdziesz tutaj: https://huggingface.co/EleutherAI/gpt-neo-1.3B.\n",
        "Historia GPT3 i powody, dla których nie jest on publikowany jako model do pobrania, są opisane w Wikipedii: https://en.wikipedia.org/wiki/GPT-3.\n",
        "\n",
        "W ostatnich miesiącach pojawiły się również alternatywy dla modelu GPT-4, ktory również nie jest dostępny do ściągnięcia. Ciekawym modelem jest mini-GPT4, który można znaleźć tu: https://minigpt-4.github.io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biPqEQ2C68cV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
