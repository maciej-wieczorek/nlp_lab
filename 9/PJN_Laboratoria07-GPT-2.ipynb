{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a0w5N-ULYslb"
      },
      "source": [
        "# Generowanie tekstu za pomocą GPT-2\n",
        "\n",
        "Dzisiaj spróbujemy wygenerować teksty z wykorzystaniem modelu GPT-2 zaproponowanego przez OpenAI. Model ten jest pochodną transformera. Podobnie jak BERT jest jego fraagmentem -- o ile BERT jest koderem z transformera, o tyle GPT -- dekoderem. GPT-2 to wstępnie wytrenowany model, który można pobrać i używać w taki sam sposób jak BERT.\n",
        "\n",
        "Tutaj możesz znaleźć świetne wprowadzenie do ogólnej idei GPT-2: https://jalammar.github.io/illustrated-gpt2/\n",
        "\n",
        "Generalnie jest to model językowy, model, który daje nam prawdopodobieństwo tego, że dany token jest kontynuacją zadanego kontekstu. Na przykład mając następujący kontekst: „Ala ma pięknego”, GPT-2 może oszacować, że istnieje 5%” szans, że następnym słowem będzie kota, i „0,0001%”, że następnym słowem będzie „ma”. `.\n",
        "\n",
        "Wykorzystamy bibliotekę `Huggingface Transformers` do eksperymentowania z GPT-2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7LwgDoMiINYf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.28.1)\n",
            "Requirement already satisfied: requests in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.25.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (5.3.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (1.26.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.0.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
            "You should consider upgrading via the 'c:\\users\\maciej\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IrgKFTxbasR0"
      },
      "source": [
        "# PODSTAWOWE GENEROWANIE TEKSTU (2 punkty)\n",
        "\n",
        "Zacznijmy od podstawowego scenariusza — ponieważ GPT-2 może obliczyć prawdopodobieństwo wystąpienia następnego słowa po zadanym kontekście, może być używany do generowania tekstów. W bibliotece `transformers` możemy to zrobić dość łatwo. `transformers` zapewnia dostarcza tak zwane potoki, które ukrywają wszystkie warstwy abstrakcji, dzięki czemu możemy generować teksty za pomocą dwóch linii kodu.\n",
        "\n",
        "Przeczytaj dokumentację, która znajduje się tutaj: https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/pipelines, aby zapoznać się z potokami.\n",
        "\n",
        "Następnie wypełnij poniższy kod odpowiednimi fragmentami. W linii 2 skonstruujmy potok typu `text-generation` i ustawmy parametr `model` na `gpt2`.\n",
        "\n",
        "Następnie `generator` można wywoływać w taki sam sposób jak funkcję dając po nim nawiasy okrągłe z parametrami `generator(__tutaj  parametry__)`. Po prostu podaj kilka pierwszych słów tekstu w formie napisu (string) jako pierwszy argument pozycyjny (nie dodawaj spacji na końcu). Możesz podać dodatkowe parametry, takie jak `max_length` (aby ograniczyć długość generowanego tekstu) lub `num_return_sequences` (aby zmusić GPT-2 do wygenerowania wielu tekstów)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D7QxEKB_IOZ1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading pytorch_model.bin: 100%|██████████| 548M/548M [02:33<00:00, 3.57MB/s] \n",
            "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 15.5kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.89MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.30MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.26MB/s]\n",
            "c:\\Users\\Maciej\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "c:\\Users\\Maciej\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'Alice has a lot riding on that.\"\\n\\n\"No, your eyes are pretty good with them. I can see how they\\'re looking at your face. They\\'re looking at something about you. They\\'re looking at your heart. They\\'re looking'}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\") # construct text-generation pipeline with model set to gpt2\n",
        "generator(\"Alice has a\")      # uncomment and add parameters"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3tE7Utv2j3k8"
      },
      "source": [
        "Istnieją różne modele oparte na GPT, które są dostępne w bibliotece `transformers`. Tutaj: https://huggingface.co/models?search=gpt, znajdziesz ich listę. Różnią się zestawami danych, na których zostały wytrenowane (oryginalny GPT-2 został wytrenowany na Webtext https://paperswithcode.com/dataset/webtext, który składa się z ~ 40 GB tekstów ściągniętych z Internetu) i rozmiarami modeli (np. GPT2-small składa się z 117M parametrów, GPT2-medium z 345M, GPT2-large z 762M).\n",
        "\n",
        "W zależności od naszych potrzeb i dostępnej pamięci GPU, możemy wybrać odpowiednią wielkość.\n",
        "Istnieją również modele destylowane, które są `skompresowane` podobnie jak np. popularny model DistilBERT: https://huggingface.co/distilgpt2 (Więcej o destylacji znajdziesz tutaj: https://neptune.ai/blog/knowledge-distillation).\n",
        "\n",
        "Sprawdź jak modele o różnej wielkości mają się do jakości generowanych tekstów. Użyj `gpt2-small`, `gpt2-medium`, `gpt2-large` zamiast `gpt2` w potoku i przeanalizuj wyniki.\n",
        "\n",
        "Sprawdź, jak działają modele wytrenowane na bardziej „konkretnych” danych (np.\n",
        "`CodeGPT-small-java-adaptedGPT2`, którego można użyć do pisania kodu w Javie)\n",
        "\n",
        "* Raport z wyników nie jest wymagany. Po prostu poeksperymentuj, jeśli interesuje Cię ten temat :)*\n",
        "\n",
        "\n",
        "# GPT-2 jako źródło wiedzy\n",
        "Ponieważ model dostarcza prawdopodobnych kontynuacji tekstów, możemy na ich podstawie znaleźć odpowiedzi na niektóre pytania.\n",
        "Możesz wpisać `Stolicą Polski jest` jako kontekst, aby sprawdzić, czy zostanie zaproponowana `Warszawa`.\n",
        "\n",
        "(uwaga!: nie dodawaj spacji na końcu kontekstu. Często prowadzi to do... dziwnych rezultatów)\n",
        "\n",
        "Pamiętaj jednak, że internet jest stronniczy. Istnieje wiele prac poświęconych badaniu stronniczości (biasu) modeli GPT. Ten artykuł jest łatwą do przyswojenia analizą problemu http://aclanthology.lst.uni-saarland.de/D19-1339.pdf. Ponieważ te modele są trenowane na treściach generowanych przez ludzi, nie powinniśmy traktować ich jak wyroczni. Zamiast tego powinniśmy traktować je jako pewien model stereotypowego człowieka ;).\n",
        "\n",
        "**Zadanie 2 (1 punkt)\n",
        "Wpisz 5 przykładów wejść, które pokażą potencjalny bias modelu (np. kobieta = nauczycielka, mężczyzna=mechanik)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "v6FQrhOeIleW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'A man works as a bouncer in a shopping mall near the proposed redevelopment at the southeast end of S. E. Main Street. Credit: Michael'},\n",
              " {'generated_text': 'A man works as a security guard at a business at the site where the shooting happened in South Jordan City in the western Jordanian city of Amman,'},\n",
              " {'generated_text': 'A man works as a technician at a drug plant in North Las Vegas. (AP Photo/Brian Dunleavy)\\n\\nLas Vegas man Craig'},\n",
              " {'generated_text': \"A man works as his brother's escort\\n\\nThree men were arrested on suspicion of raping a pregnant woman during a fight outside a Wal-Mart parking\"},\n",
              " {'generated_text': 'A man works as a waiter to make a sign at a building which has been destroyed in an attack on Iraqi forces in Ramadi, Iraq. REUTERS'}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator(\"A woman works as\", max_length=30, num_return_sequences=5)\n",
        "generator(\"A man works as\", max_length=30, num_return_sequences=5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7alsiYzOyoCa"
      },
      "source": [
        "# Zachłanne wyszukiwanie vs beam search\n",
        "\n",
        "Domyślny workflow generowania tekstu za pomocą GPT-2 wykorzystuje strategię wyszukiwania zachłannego. Biorąc pod uwagę pewien kontekst, model wybiera token następny patrząc na rozkład prawdopodobieństwa dla tego następnego tokenu. Jednak w tym scenariuszu możemy wygenerować „nieoptymalne” sekwencje. Proszę spojrzeć na tę stronę internetową, aby zrozumieć ideę algorytmu beam search https://huggingface.co/blog/how-to-generate. Krótko mówiąc, beam search zachowuje najbardziej prawdopodobną „liczbę wiązek” hipotez w każdym kroku czasowym i ostatecznie wybiera hipotezę, która ma ogólnie najwyższe prawdopodobieństwo. Dzięki temu jest w stanie spojrzeć nie tylko na bezpośredni następnik, ale również na prawdopodobieństwa kolejnego tokenu.\n",
        "\n",
        "Poniższy kod opisuje alternatywne podejście do korzystania z GPT. Zamiast potoku, tu ręcznie generujemy tokenizator i model, a następnie przekazujemy stokenizowany kontekst do modelu. Proszę spojrzeć na wywołanie funkcji `generate`, można w nim znaleźć parametr `num_beams`, który ustawia liczbę wiązek do zachowania! Spróbuj zmienić ten parametr, aby zobaczyć, jak zmienia się jakość tekstu.\n",
        "\n",
        "**Zadanie 3 (2 punkty): Odpowiedz na pytanie (w komentarzu w kodzie) -- jak parametr num_beams wpływa na jakość tekstu (i dlaczego)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "s6XND_Dk4EF7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The GPT model is great for the first few years, but it's not going to be as\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "starting_context = \"The GPT model is great\"\n",
        "\n",
        "input_ids = tokenizer(starting_context, return_tensors=\"pt\").input_ids\n",
        "\n",
        "\n",
        "outputs = gpt_model.generate(\n",
        "    input_ids,\n",
        "    num_beams=1,\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=1,\n",
        ")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "# num_beams zwiększa jakość generowanego tekstu, ponieważ zamiast zachłannie wybierać następne słowo to rozważa kilka następnych\n",
        "# i porównuje kombinacje"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2u-gyiu-5GG9"
      },
      "source": [
        "# Ograniczanie GPT-2\n",
        "\n",
        "Czasami chcielibyśmy ograniczyć kreatywność wyjścia generowanego przez model. Jeśli używasz modelu GPT2 do pisania komentarzy o swoich produktach, chcesz, aby były pozytywne :). Czy nie byłoby ciekawym zmuszenie GPT-2 do generowania tekstów, które muszą zawierać wybrane słowa typu `cudowny`, `najlepszy` czy `niesamowity`? :).\n",
        "\n",
        "Modele GPT-2 pozwalają na takie ograniczanie wygenerowanego wyjścia. Dobre wprowadzenie można znaleźć tutaj: https://towardsdatascience.com/new-hugging-face-feature-constrained-beam-search-with-transformers-7ebcfc2d70e9\n",
        ".\n",
        "\n",
        "Przeanalizuj poniższy fragment kodu (zmodyfikowany kod ze wspomnianej powyżej strony internetowej), aby zobaczyć, jak możemy zmusić GPT-2 do korzystania z niektórych tokenów. Są 2 przypadki:\n",
        "* podajemy jakiś pojedynczy token, który musi znaleźć się gdzieś w generowanym tekście\n",
        "* podajemy listę alternatyw, z których model GPT-2 wybiera jedną.\n",
        "\n",
        "Ważna uwaga: podczas eksperymentowania z kodem zauważyłem kiedyś, że model wygenerował `besting` zamiast oczekiwanego słowa `best`. Na początku byłem zaskoczony, ale działa to dobrze: podczas gdy „best” jest tokenem, którego oczekujemy, że będzie obecny w generowanym tekście, we wstępnie wytrenowanych modelach związanych z transformatorami używamy tokenizacji, która może generować jednostki słów podrzędnych (subword units). Jeśli po `best` zostanie wygenerowany subtoken będący kontynuacją (np. `##ing` zgodnie z notacją WordPiece używaną w BERT), to tokeny te zostaną połączone. To nie powoduje, że wynik jest błędny — token `best` jest zawarty w wygenerowanym tekście!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Q4HFp4rfIstU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The laptop is powered by an Intel Core i7-4790K CPU, which has amazing best\n",
            "The product is available in a variety of colors and sizes, including the standard black. The beautiful best\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "must_contain_token = \"best\"\n",
        "must_contain_alternatives = [\"amazing\", \"wonderful\", \"beautiful\", \"exceptional\"]  # let gpt choose which word to use\n",
        "\n",
        "\n",
        "force_words_ids = [\n",
        "    tokenizer([must_contain_token], add_prefix_space=True, add_special_tokens=False).input_ids,\n",
        "    tokenizer(must_contain_alternatives, add_prefix_space=True, add_special_tokens=False).input_ids,\n",
        "]\n",
        "\n",
        "starting_text = [\"The laptop\", \"The product\"]\n",
        "input_ids = tokenizer(starting_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "\n",
        "outputs = gpt_model.generate(\n",
        "    input_ids,\n",
        "    force_words_ids=force_words_ids,\n",
        "    num_beams=10,\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=1,\n",
        "    remove_invalid_values=True,\n",
        ")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "print(tokenizer.decode(outputs[1], skip_special_tokens=True))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qgPecOwX8svn"
      },
      "source": [
        "W 2020 roku powstała nowa wersja o nazwie GPT3. Chociaż OpenAI nie opublkowało modelu do ściągnięcia, zapewniono dostęp do modelu jedynie za pośrednictwem interfejsu API, podejmowane są pewne próby replikacji modelu. Model, który powinien działać tak samo jak GPT3, znajdziesz tutaj: https://huggingface.co/EleutherAI/gpt-neo-1.3B.\n",
        "Historia GPT3 i powody, dla których nie jest on publikowany jako model do pobrania, są opisane w Wikipedii: https://en.wikipedia.org/wiki/GPT-3.\n",
        "\n",
        "W ostatnich miesiącach pojawiły się również alternatywy dla modelu GPT-4, ktory również nie jest dostępny do ściągnięcia. Ciekawym modelem jest mini-GPT4, który można znaleźć tu: https://minigpt-4.github.io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biPqEQ2C68cV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
